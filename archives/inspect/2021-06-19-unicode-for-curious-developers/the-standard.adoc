
[[sect-standard]]
== The Standard

Unicode can be used informally to refer to the _Unicode Standard_, the encoding standard for characters, and the _Unicode Consortium_, the organization that created the Unicode Standard. We have already talked about the consortium in the last part. We will now look at the Unicode Standard, and the next part will focus on its implementation in modern systems.


[[sect-standard-standard]]
=== The Unicode Standard

The Unicode Standard is the reference document and is link:http://www.unicode.org/versions/latest/[freely accessible online] (the link:https://www.amazon.com/Unicode-Standard-Version-5-0-5th/dp/0321480910/[last edition published as a book] dates back to 2006, and recent editions are available as Print-on-Demand (POD) for purchase on link:https://www.lulu.com/en/us/shop/unicode-consortium/the-unicode-standard-version-130-volume-1/paperback/product-qkgep6.html[lulu.com]). The link:https://www.unicode.org/versions/Unicode13.0.0/UnicodeStandard-13.0.pdf[current PDF version] contains more than 1000 pages, but only the first 300 pages define Unicode and are a must-read if you want to go deeper on the subject. The remaining pages examine all supported scripts one by one.

The main contribution of the Unicode Standard is the _Unicode Character Table_. *Unicode specifies a numeric value (code point) and a name for every defined
character*. In this respect, it is similar to other character encoding standards from ASCII
onward. But the Unicode Standard is far more than a simple encoding of characters. *Unicode also associates a rich set of semantics with each encoded character*‚Äîproperties like case and directionality required for interoperability and correct behavior in implementations. These semantics are cataloged in the _Unicode Character Database_, a collection of data files discussed at length in the last part.

*The Unicode Character Table has an overall capacity for more than 1 million characters*, which is more than sufficient for all the world's languages, but also for currency symbols, punctuation marks, mathematical symbols, technical symbols, geometric shapes, dingbats, and emojis. The Unicode Standard does not encode personal characters like logos but reserves 6,400 code points for private use in the Basic Multilingual Plane (BMP), the first 65,536 code points, and more than 100,000 private code points overall.

*A _character_ is an abstract entity*, such as ‚Äúlatin capital letter a‚Äù or ‚Äúbengali digit five,‚Äù defined by a unique code point, such as U+0041 or U+09EB. These code points are integers represented in base 16, ranging from 0 to 10FFFF (called the _codespace_). The representation of a character on screen, called a _glyph_, depends on the software implementation, based on the fonts available on your device. The Unicode Standard does not specify the precise shape, size, or orientation of on-screen characters. *Unicode characters are code points, not glyphs*. Unicode declares that U+0041 is A, U+03B1 is Œ±, etc. Simple?

BUT.

* Unicode includes _compatibility characters_, which would not be included if they were not present in other standards.footnote:[Unicode includes link:https://scripts.sil.org/cms/scripts/page.php?site_id=nrsi&id=IWS-Chapter04b[more than 10,000 characters that compromise the design principles behind the Standard], like unification or dynamic composition.] The idea is to make it easy to have a one-to-one mapping between the Unicode Standard and other standards. For example, U+2163 ‚ÄúIV‚Äù is the roman numeral four even if we could have used two existing characters to represent it.
* Unicode includes _combining characters_, which are characters to be positioned relative to the previous base characters. Accents are implemented using combining characters, so U+0065 e + U+0302  ÃÇ ‚Üí √™, and symbols can be composed using the same mechanism, so:
+
image::unicode-standard-figure-2-17.png[title="Combining Enclosing Marks for Symbols (From Unicode Standard, Figure 2.17)", width=300]
+
Unicode does not restrict the number of combining characters that may follow a base character, but implementations are not obliged to support all of them.
+
image::unicode-standard-figure-2-21.png[title="Stacking Sequences (From Unicode Standard, Figure 2.21)", width=500]
* Unicode recognizes _equivalent sequences_, which are two sequences using different Unicode codes to represent the same character.
+
image::unicode-standard-figure-2-23.png[title="Equivalent Sequences (From Unicode Standard, Figure 2.23)", width=400]
* Unicode includes _special characters_. For example, UTF-16 and UTF-32 are sensible to the endianness of the hardware and often include a Byte Order Mark (BOM), which is composed of two code points‚ÄîU+FEFF `zero width no-break space` and U+FFFE (a special character). Implementations can detect which byte ordering is used in a file based on the order of these two code points. Another common example is the U+FFFD `replacement character` ÔøΩ used to represent ‚Äúunknown‚Äù characters.

We will talk more about these particularities when covering the implementation.



[[sect-standard-character-table]]
=== The Unicode Character Table


The link:https://unicode.org/charts/[Unicode Character Table] has room for more than one million characters. That's a lot! Their position in this table means nothing.  (There are small differences like the number of bytes to represent them in the various encodings but from a logical perspective, all characters behave similarly.) In theory, the code point of a character only needs to be unique, but in practice, its assignment follows some "conventions." Marie Kondo would probably appreciate the effort to organize the characters.

*The codespace is divided up into 17 _planes_ of characters*‚Äîeach plane consisting of 64K code points. Not all planes are currently in use.

* Plane 0: the _Basic Multilingual Plane_ (BMP) is the main descendant of the first version of Unicode (with the 16-bits limitation), and the majority of frequent characters can be found here.
* Plane 1: the _Supplementary Multilingual Plane_ (SMP) is the extension of the BPM for scripts or symbols with very infrequent usage. Most emojis are present in Plane 1, but as always, exceptions exist üòâ.
* Plane 2: the _Supplementary Ideographic Plane_ (SIP) is similar to Plane 1 but for rare  CJK characters. (CJK is a collective term for the Chinese, Japanese, and Korean languages.)
* Planes 3..13 are ... empty.
* Plane 14: the _Supplementary Special-purpose Plane_ (SSP) is the spillover allocation area for format control characters that do not fit into the small allocation areas for format control characters in the BMP.
* Planes 15..16: the _Private Use Planes_. These code points can be freely used for any purpose, but their use requires that the sender and receiver agree on their interpretation.

*Planes are further divided into subparts called _blocks_*. Character blocks generally contain characters from a single _script_, and in many cases, a script is fully represented in its block.

image::unicode-allocation.png[title="Unicode allocation", width=750]

These blocks and scripts are also used to organize the link:https://unicode.org/charts/[Unicode Code Charts] in the documentation so that you can quickly jump to the given script of your language. If you want the full listing instead, you can download link:https://www.unicode.org/Public/UCD/latest/charts/CodeCharts.pdf[the complete code charts in PDF] (2684 pages and 110 MB! üòÖ).

image::unicode-code-charts.png[title="Unicode Character Code Charts", link=https://www.unicode.org/charts/#scripts]



[[sect-standard-character-database]]
=== The Unicode Character Database

The link:https://www.unicode.org/reports/tr44[Unicode Character Database] (UCD) is a set of documentation and data files link:https://www.unicode.org/Public/UCD/latest/ucd/[accessible online]. These files contain more than 100 character properties, including:

* A name +
  ‚Üí Useful to refer to a character using a unique identifier instead of a hexadecimal value, like using the name `tab` instead of U+0009.
* The general category (basic partition into letters, numbers, symbols, punctuation, and so on). +
  ‚Üí Useful to determine the primary use (letter, digit, punctuation, symbol) when implementing functions like `isDigit()`.
* Some general characteristics (whitespace, dash, ideographic, alphabetic, noncharacter, deprecated, and so on) +
  ‚Üí Useful to determine the kind of character like digits.
* Some display-related properties (bidirectional class, shaping, mirroring, width, and so on) +
  ‚Üí Useful when rendering the text on screen.
* The case (upper, lower, title, folding‚Äîboth simple and full) +
  ‚Üí Useful to determine if a character is uppercase.
* The script and block a character belongs. +
  ‚Üí Useful to find characters commonly used together.
* and a lot more!

You can visualize these properties from many websites like link:https://unicode-table.com/[unicode-table.com]:

image::unicode-table-character-A.png[]

These websites are based on the files available in the UCD. We will present the main ones in this article. These files follow a few conventions: each line consists of fields separated by semicolons, the first field represents a code point or range expressed as hexadecimal numbers. The remaining fields are properties associated with that code point. A code point may be omitted in a data file if the default value for the property in question applies.

`UnicodeData.txt` is the main file:

[source]
.UnicodeData.txt
----
...
0009;<control>;Cc;0;S;;;;;N;CHARACTER TABULATION;;;;
‚Ä¶
0021;EXCLAMATION MARK;Po;0;ON;;;;;N;;;;;
‚Ä¶
0041;LATIN CAPITAL LETTER A;Lu;0;L;;;;;N;;;;0061;
...
----

Where (link:https://www.unicode.org/reports/tr44/#UnicodeData.txt[see full details]):

* `0041` is the code point (U+0041).
* `LATIN CAPITAL LETTER A` is the property `NAME`.
* `Lu` is the abbreviation for the value `Uppercase_Letter` of the property `General_Category`.
* `L` is the abbreviation for the value `Left_To_Right` of the property `Bidi_Class` to indicate a left-to-right character. `ON` stands for `Other_Neutral` and is used by most punctuation characters.
* `0061` is the code point value of the property `Simple_Lowercase_Mapping` which means the lowercase character for U+0041 `A` is U+0061 `a`.

`emoji-data.txt` is the main file concerning emojis:

[source]
.emoji/emoji-data.txt
----
...
1F600         ; Emoji                # E1.0   [1] (üòÄ)       grinning face
1F601..1F606  ; Emoji                # E0.6   [6] (üòÅ..üòÜ)    beaming face with smiling eyes..grinning squinting face
1F607..1F608  ; Emoji                # E1.0   [2] (üòá..üòà)    smiling face with halo..smiling face with horns
1F609..1F60D  ; Emoji                # E0.6   [5] (üòâ..üòç)    winking face..smiling face with heart-eyes
1F60E         ; Emoji                # E1.0   [1] (üòé)       smiling face with sunglasses
1F60F         ; Emoji                # E0.6   [1] (üòè)       smirking face
1F610         ; Emoji                # E0.7   [1] (üòê)       neutral face
1F611         ; Emoji                # E1.0   [1] (üòë)       expressionless face
...
----

Where (link:https://www.unicode.org/reports/tr44/#emoji-data.txt[see full details]):

* `Emoji` is the default type. Other possible values are `Emoji_Modifier` for the skin tone modifier, `Emoji_Modifier_Base` for characters that can serve as a base for emoji modifiers. `Emoji_Component` for characters used in emoji sequences like flags.
* The comment indicates the first version that introduced the emoji(s), the count of emojis in the range, a preview of the emoji(s), and their name(s).


[[sect-standard-encodings]]
=== The Unicode Encodings

Character encodings are necessary when exchanging or storing texts. Computers only understand ``0``s and ``1``s and therefore, Unicode code points must be converted into binary.

The Unicode Standard provides three distinct encoding forms for Unicode characters, using minimum 8-bit, 16-bit, and 32-bit units. These are named UTF-8, UTF-16, and UTF-32, respectively. *All three encoding forms can be used to represent the full range of Unicode characters and each one can be efficiently transformed into either of the other two without any loss of data*.

image::unicode-standard-figure-2-11.png[title="Unicode Encoding Forms (From Unicode Standard, Figure 2.11)", width=500]

[NOTE]
.The Principle of Nonoverlapping
====
Unicode encodings differ from many prior encodings that also use varied-length bytes but where overlap was permitted. For example:

image::unicode-standard-figure-2-9.png[title="Overlap in Legacy Mixed-Width Encodings (From Unicode Standard, Figure 2.9)", width=500]

To determine the character, these encodings depend on the first byte. If someone
searches for the character ‚ÄúD,‚Äù for example, he might find it in the trail byte of the two-byte sequence –î. The program must look backward through text to find the correct matches, but the boundaries are not always easy to interpret with overlapping:

image::unicode-standard-figure-2-10.png[title="Boundaries and Interpretation (From Unicode Standard, Figure 2.10)", width=500]

Therefore, each of the Unicode encoding forms is designed with the principle of nonoverlapping in mind to make implementations more simple and more efficient.
====


[[sect-standard-encodings-utf32]]
==== UTF-32

UTF-32 is the simplest Unicode encoding form. UTF-32 is a fixed-width character encoding form. *Each Unicode code point is represented directly by a single 32-bit code unit*. Because of this, UTF-32 has a one-to-one relationship between encoded character and code unit;

Note that 32 bits have space for more than four million codes, but UTF-32 restricts the representation of code points in the standard ranges 0..10FFFF (we need to cover UTF-16 first to explain this restriction).

[source,go]
----
import (
	"bytes"
	"encoding/binary"
)

func EncodeUTF32BE(codepoints []uint32) []byte {
	buf := new(bytes.Buffer)

	// BOM (optional)
	binary.Write(buf, binary.BigEndian, uint32(0xFEFF))

	for _, codepoint := range codepoints {
		// Each codepoint is written as unit32
		binary.Write(buf, binary.BigEndian, codepoint)
	}

	return buf.Bytes()
}
----

Decoding follows the inverse logic:

. Read the next four bytes.
. Extract the value to get the code point.
. Repeat.

*Preferred Usage*: UTF-32 may be preferred where memory or disk storage space is not limited and when the simplicity of access to single code units is desired. For example, the first version of Python 3 represented strings as sequences of Unicode code points, but Python 3.3 changed the implementation to optimize the memory usage.


[[sect-standard-encodings-utf16]]
==== UTF-16

*UTF-16 is the historical descendant of the earliest form of Unicode where only 16-bits were used for code points*. The characters in the range U+0000..U+FFFF (the first 65,536 characters) are often called the Basic Multilingual Plane (BMP, or Plane 0), and are encoded as a single 16-bit code unit using the code point value like in UTF-32.

The remaining characters are called _surrogates_ and are encoded as pairs of 16-bit code units whose values are disjunct from the code units used for the single code unit representations, thus maintaining non-overlap for all code point representations in UTF-16.

[NOTE]
.Understanding the maximum number of code points in Unicode
====
Not breaking already encoded texts in UCS-2 (known as Unicode at that time) was the biggest challenge to extend the initial number of Unicode characters (2^16^ = 65536 characters).

The solution (now called UTF-16) is to rely on two unused ranges 0xD800..0xDBFF and 0xDC00‚Äì0xDFFF (each one representing 1024 code points). If we concatenate these two ranges, it means we can represent 1024 * 1024 = 1,048,576 new characters, in addition to the 63488 original code points (2^16^ - 2048, the number of unique code points that fits 2 bytes minus the ranges previously unused).

image::unicode-codespace-size-limit.png[title="Unicode surrogate codes", width=600]

So, in UTF-16, the representation for all initial characters does not change. But when we need to encode one of the new characters, we will not use their code points--they just cannot fit in a 16-byte word--but use instead what is called a surrogate pair, which is a pair of pointers to retrieve the original code point. If we consider the binary representation of the two previously unused ranges:

[source]
----
0xD800 = 0b1101100000000000 (110110 = high surrogate prefix)
0xDFFF = 0b1101111111111111 (110111 =  low surrogate prefix)
----

Every new code point is represented in UTF-16 by two 16-bytes words‚Äîthe high surrogate followed by the low surrogate‚Äîeach one uses one of the two ranges. Note that six bits are reserved for the prefix of the ranges. Therefore, if a byte starts with `110110`, we know we have a high surrogate that is followed by a low surrogate starting with `110111`, and inversely. It means we have 20 representative bits to represent the new characters (2^20^ = 1,048,576).

image::surrogate-pair-decoding.png[title="Decoding surrogate pairs in UTF-16", width=600]

Even if other encodings like UTF-32 and UTF-8 can represent more code points, the total number of valid Unicode code points is still constrained by UTF-16 for backward compatibility reasons. The exact number is link:https://www.johndcook.com/blog/2019/09/02/number-of-possible-unicode-characters[1,111,998 possible Unicode characters], a little less than our estimation due to 2 reserved characters at the end of each plane.
====


[source,go]
----
import (
	"bytes"
	"encoding/binary"
)

func EncodeUTF16BE(codepoints []uint32) []byte {
	// Code is inspired by Go official implementation of module unicode/utf16
	// https://github.com/golang/go/blob/go1.16/src/unicode/utf16/utf16.go

	buf := new(bytes.Buffer)

	// BOM
	binary.Write(buf, binary.BigEndian, uint16(0xFEFF))
	for _, v := range codepoints {
		switch {
		case v < 0x10000:
			// Code points in the Basic Multilingual Plane (BMP)
			// are written as such in uint16 as they can safely
			// be stored in two bytes.
			binary.Write(buf, binary.BigEndian, uint16(v))
		case 0x10000 <= v:
			// Code points in Supplementary Planes are encoded
			// as two 16-bit code units called a surrogate pair.

			// 0x10000 is subtracted from the code point,
			// leaving a 20-bit number in the hex number range 0x00000‚Äì0xFFFFF
			r := v - 0x10000

			// The high ten bits (in the range 0x000‚Äì0x3FF) are added to 0xD800
			// to give the first 16-bit code unit or high surrogate,
			// which will be in the range 0xD800‚Äì0xDBFF.
			r1 := 0xd800 + (r>>10)&0x3ff
			binary.Write(buf, binary.BigEndian, uint8(r1))

			// The low ten bits (also in the range 0x000‚Äì0x3FF) are added
			// to 0xDC00 to give the second 16-bit code unit or low surrogate,
			// which will be in the range 0xDC00‚Äì0xDFFF.
			r2 := 0xdc00 + r&0x3ff
			binary.Write(buf, binary.BigEndian, uint8(r2))
		}
	}

	return buf.Bytes()
}
----

Decoding simply needs to test for surrogates:

. Read the next two bytes.
. If the value is in the range U+0000..U+FFFF, this is a code point.
. Otherwise, retrieve the value from the high surrogate (0xD800..0xDBFF) and the low surrogate (0xDC00‚Äì0xDFFF) by reading two more bytes and extract the code point using basic mathematical operations.

*Preferred Usage*: UTF-16 provides a balanced representation that is reasonably compact as all the common, heavily used characters fit into a single 16-bit code unit. This encoding is often used by programming languages as their internal representation of strings for that reason, but for file encoding, UTF-8 is by far the most privileged encoding.


[[sect-standard-encodings-utf8]]
==== UTF-8

UTF-8 is a variable-width like UTF-16, but offering compatibility with ASCII. That means Unicode code points U+0000..U+007F are converted to a single byte 0x00..0x7F in UTF-8 and are thus indistinguishable from ASCII itself. An ASCII document is a valid UTF-8 document (the reverse is rarely true).

|===
| First code point | Last code point | Byte 1 | Byte 2 | Byte 3 | Byte 4

| U+0000
| U+007F
| 0xxxxxxx
|
|
|

| U+0080
| U+07FF
| **11**0xxxxx
| 10xxxxxx
|
|

| U+0800
| U+FFFF
| **111**0xxxx
| 10xxxxxx
| 10xxxxxx
|

| U+10000
| U+10FFFF
| **1111**0xxx
| 10xxxxxx
| 10xxxxxx
| 10xxxxxx

|===

Note that when the first byte starts with `1`, the number of successive ``1``s gives the number of bytes for this code point.


[source,go]
----
import (
	"bytes"
	"encoding/binary"
)

func EncodeUTF8(codepoints []uint32) []byte {
	// Code is inspired by Go official implementation of module unicode/utf8
	// https://github.com/golang/go/blob/go1.16/src/unicode/utf8/utf8.go

	buf := new(bytes.Buffer)

	// Note: The Unicode Standard neither requires nor recommends
	// the use of the BOM for UTF-8.

	for _, r := range codepoints {
		switch i := uint32(r); {

		// 1 byte for ASCII characters
		case int(r) <= 0x007F: // 127
			buf.WriteByte(byte(r)) // 0xxxxxxx

		// 2 bytes for most Latin scripts
		case i <= 0x07FF: // 2047
			buf.WriteByte(0b11000000 | byte(r>>6))         // 110xxxxx
			buf.WriteByte(0b10000000 | byte(r)&0b00111111) // 10xxxxxx

		// 3 bytes for the rest of the BMP
		case i <= 0xFFFF: // 65535
			buf.WriteByte(0b11100000 | byte(r>>12))           // 1110xxxx
			buf.WriteByte(0b10000000 | byte(r>>6)&0b00111111) // 10xxxxxx
			buf.WriteByte(0b10000000 | byte(r)&0b00111111)    // 10xxxxxx

		// 4 bytes for other planes and most emojis
		default:
			buf.WriteByte(0b11110000 | byte(r>>18))            // 11110xxx
			buf.WriteByte(0b10000000 | byte(r>>12)&0b00111111) // 10xxxxxx
			buf.WriteByte(0b10000000 | byte(r>>6)&0b00111111)  // 10xxxxxx
			buf.WriteByte(0b10000000 | byte(r)&0b00111111)     // 10xxxxxx
		}
	}

	return buf.Bytes()
}
----

Decoding is easy to implement.

. Read the next byte.
. If it starts by `0`, the character is encoded using 1 byte.
. If it starts by `110`, the character is encoded using 2 bytes. (two leading ``1``s)
. If it starts by `1110`, the character is encoded using 3 bytes. (three leading ``1``s)
. If it starts by `11110`, the character is encoded using 4 bytes. (four leading ``1``s)
. Read the remaining bits of the first byte.
. Read the last six bits of the other composing byte(s).
. Reassemble using basic mathematical operations to retrieve the code unit.
. Repeat.

*Preferred Usage*: UTF-8 is particularly compact when the text contains mainly ASCII characters, which is often the case for a large percent of the population, but UTF-8 is significantly larger for Asian writing systems compared to UTF-16 as these characters require three bytes in UTF-8.

In practice, UTF-8 has become the default Unicode encoding of the Web even if all three encodings are perfectly valid.
